init:
  _target_: mbrlax.models.lllaplace.init
  network: ${network}
predict_fn:
  _target_: mbrlax.models.laplace.build_predict_fn
train_fn:
  _target_: mbrlax.models.laplace.build_train_fn
  num_epochs: 80000
  batch_size: 64
  optimizer:
    _target_: optax.adam
    learning_rate: 1e-4
  posterior_fn:
    _target_: mbrlax.models.lllaplace.posterior_fn
    _partial_: true
  key:
    _target_: jax.random.PRNGKey
    seed: ${experiment.random_seed}
  early_stop:
    _target_: flax.training.early_stopping.EarlyStopping
    min_delta: 0
    patience: 500
#
#
# _target_: mbrlax.models.laplace.lllaplace.build_model
# network: ${network}
# num_epochs: 80000
# batch_size: 64
# optimizer:
#   _target_: optax.adam
#   learning_rate: 1e-4
# key:
#   _target_: jax.random.PRNGKey
#   seed: ${experiment.random_seed}
# early_stop:
#   _target_: flax.training.early_stopping.EarlyStopping
#   min_delta: 0
#   patience: 500

#   _target_: mbrlax.models.lllaplace.LLLaplace
#   network: ${nwork}
#   input_dim: ${input_dim}
# training:
#   loss:
#     _target_: mbrlax.models.mlp.loss
#     _partial_: true
#   # min_delta: 1e-3
#   min_delta: 0
#   # patience: 5
#   patience: 500
#   # patience: 500
#   num_epochs: 80000
#   # num_epochs: 8
#   batch_size: 64
#   # num_epochs: 80
#   # logging_epoch_freq: 1000 # monitoring config
#   save: True
# model:
#   _target_: examples.train.BayesianLastLayerMLP
#   # features: [2, 3, 4, 5, 6]
#   features: [10, 10, 10, 10, 6]
# training:
#   optimizer:
#     _target_: optax.adam
#     learning_rate: 1e-4
#   loss:
#     _target_: mbrlax.models.mlp.loss
#     _partial_: true
#   # min_delta: 1e-3
#   min_delta: 0
#   # patience: 5
#   patience: 500
#   # patience: 500
#   num_epochs: 80000
#   # num_epochs: 8
#   batch_size: 64
#   # num_epochs: 80
#   # logging_epoch_freq: 1000 # monitoring config
#   save: True
